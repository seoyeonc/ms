{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4713598-d70d-4c2d-8cfa-0d091303bbeb",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "# 강화학습 공부\n",
    "> \n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연\n",
    "- categories: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb3b46d-c029-4875-83b5-82f1d322d162",
   "metadata": {
    "id": "9d3f16ce-ab0a-4e35-994f-dc084c2f9e46"
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "def gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452df922-22de-4399-bda8-181f40fae46f",
   "metadata": {},
   "source": [
    "#### 지도학습(Supervised learning) vs 비지도학습(Unsupervised learning) vs 강화학습(Reinforcement learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8249279-432d-4975-87da-6cb785a66a53",
   "metadata": {
    "id": "d38fb03d-981f-456e-b12a-34a732ea7ce9",
    "outputId": "534db8a5-bf40-4b15-f544-47bb84180afd"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (20211209.0339)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"887pt\" height=\"206pt\"\n",
       " viewBox=\"0.00 0.00 886.74 206.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 202)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-202 882.74,-202 882.74,4 -4,4\"/>\n",
       "<!-- Machine learning -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Machine learning</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"91.64\" cy=\"-72\" rx=\"91.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.64\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">Machine learning</text>\n",
       "</g>\n",
       "<!-- Supervised learning -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Supervised learning</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"422.52\" cy=\"-126\" rx=\"104.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"422.52\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">Supervised learning</text>\n",
       "</g>\n",
       "<!-- Machine learning&#45;&gt;Supervised learning -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Machine learning&#45;&gt;Supervised learning</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M160.13,-83.97C173.75,-86.34 187.97,-88.78 201.28,-91 245.02,-98.3 293.71,-106.07 334.17,-112.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"333.85,-115.93 344.27,-114.02 334.93,-109.01 333.85,-115.93\"/>\n",
       "<text text-anchor=\"middle\" x=\"242.78\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">(x,y),loss fn</text>\n",
       "</g>\n",
       "<!-- Unsupervised learning -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Unsupervised learning</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"422.52\" cy=\"-72\" rx=\"116.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"422.52\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">Unsupervised learning</text>\n",
       "</g>\n",
       "<!-- Machine learning&#45;&gt;Unsupervised learning -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Machine learning&#45;&gt;Unsupervised learning</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M183.38,-72C218.05,-72 258.23,-72 295.61,-72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296.06,-75.5 306.06,-72 296.06,-68.5 296.06,-75.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"242.78\" y=\"-75.8\" font-family=\"Times,serif\" font-size=\"14.00\">(x)</text>\n",
       "</g>\n",
       "<!-- Reinforcement learning -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Reinforcement learning</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"422.52\" cy=\"-18\" rx=\"120.48\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"422.52\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Reinforcement learning</text>\n",
       "</g>\n",
       "<!-- Machine learning&#45;&gt;Reinforcement learning -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Machine learning&#45;&gt;Reinforcement learning</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M160.13,-60.03C173.75,-57.66 187.97,-55.22 201.28,-53 242.91,-46.05 289.02,-38.68 328.25,-32.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"329,-35.92 338.33,-30.91 327.91,-29.01 329,-35.92\"/>\n",
       "<text text-anchor=\"middle\" x=\"242.78\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\">loss fn</text>\n",
       "</g>\n",
       "<!-- Classification -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Classification</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"729.25\" cy=\"-180\" rx=\"72.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"729.25\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\">Classification</text>\n",
       "</g>\n",
       "<!-- Supervised learning&#45;&gt;Classification -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Supervised learning&#45;&gt;Classification</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M496.13,-138.87C545.59,-147.63 610.69,-159.17 659.35,-167.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"659.01,-171.29 669.47,-169.58 660.24,-164.39 659.01,-171.29\"/>\n",
       "</g>\n",
       "<!-- Regression -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Regression</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"729.25\" cy=\"-126\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"729.25\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">Regression</text>\n",
       "</g>\n",
       "<!-- Supervised learning&#45;&gt;Regression -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>Supervised learning&#45;&gt;Regression</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M527.54,-126C569.96,-126 617.95,-126 656.37,-126\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"656.43,-129.5 666.43,-126 656.43,-122.5 656.43,-129.5\"/>\n",
       "</g>\n",
       "<!-- Clustering -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>Clustering</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"729.25\" cy=\"-72\" rx=\"59.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"729.25\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">Clustering</text>\n",
       "</g>\n",
       "<!-- Unsupervised learning&#45;&gt;Clustering -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Unsupervised learning&#45;&gt;Clustering</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M539.16,-72C579.62,-72 623.75,-72 659.19,-72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"659.42,-75.5 669.42,-72 659.42,-68.5 659.42,-75.5\"/>\n",
       "</g>\n",
       "<!-- Principal Component Analysis -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>Principal Component Analysis</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"729.25\" cy=\"-18\" rx=\"149.47\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"729.25\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Principal Component Analysis</text>\n",
       "</g>\n",
       "<!-- Unsupervised learning&#45;&gt;Principal Component Analysis -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>Unsupervised learning&#45;&gt;Principal Component Analysis</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M499.63,-58.51C540.52,-51.27 591.28,-42.27 634.57,-34.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"635.31,-38.02 644.54,-32.83 634.09,-31.13 635.31,-38.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7f958822ebe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse\n",
    "gv(''' \n",
    "    \"Machine learning\" -> \"Supervised learning\"[label=\"(x,y),loss fn\"]\n",
    "    \"Machine learning\" -> \"Unsupervised learning\"[label=\"(x)\"]\n",
    "    \"Machine learning\" -> \"Reinforcement learning\"[label=\"loss fn\"]\n",
    "    \"Supervised learning\" -> \"Classification\"[label=\"\"]\n",
    "    \"Supervised learning\" -> \"Regression\"[label=\"\"]\n",
    "    \"Unsupervised learning\" -> \"Clustering\"[label=\"\"]\n",
    "    \"Unsupervised learning\" -> \"Principal Component Analysis\"[label=\"\"]\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84941a-2228-4ec1-83be-bec9f77a3b82",
   "metadata": {},
   "source": [
    "- in Supervised Learning, the goal is to **generate formula** based on input and output values. \n",
    "    - 입출력값으로 수식을 만드려는 지도학습\n",
    "- In Unsupervised Learning, we find an **association** between input values and **group** them. \n",
    "    - 입력값으로 그룹화하거나 그 사이 관계를 찾으려는 비지도학습\n",
    "- In Reinforcement Learning an agent learn through delayed feedback by **interacting with the environment**.\n",
    "    - 환경과 상호작용으로 보상을 통해 배워가려는 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a39d48-5d91-4108-8fc1-afa8686e986f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18017c-b6b1-4128-b846-77530732e72a",
   "metadata": {},
   "source": [
    "https://www.gocoder.one/blog/reinforcement-learning-project-ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2db28-889e-40e7-8e17-ac55faa1fcf6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660d36a-f980-45e3-9fe5-5cefb9f401ee",
   "metadata": {},
   "source": [
    "toy example ref: https://www.gocoder.one/blog/rl-tutorial-with-openai-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9a9e91-25f6-4dc5-9b79-c6f9a3ccee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b479f-caa2-4841-8692-77cfafd96f8f",
   "metadata": {},
   "source": [
    "create Taxi environment\n",
    "The following snippet will import the necessary packages, and create the Taxi environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5503ad7-90df-4732-9574-7b6d2b008750",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13cc187-c705-4458-9d16-a407dc10e96e",
   "metadata": {},
   "source": [
    "create a new instance of taxi, and get the initial state\n",
    "Note: Yellow = taxi, Blue letter = pickup location, Purple letter = drop-off destination\n",
    "To get the initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc88625-bb48-4935-aae9-749a1cbb05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968072e-be2d-4cf4-86da-553524cf6b2d",
   "metadata": {},
   "source": [
    "Next, we'll run a for-loop to cycle through the game. At each iteration, our agent will:\n",
    "1. Make a random action from the action space (0 - south, 1 - north, 2 - east, 3 - west, 4 - pick-up, 5 - drop-off)\n",
    "    - 행동 공간에서 임의의 행동 하기(0-남쪽, 1-북쪽, 3-서쪽, 4-픽업,5-하차)\n",
    "2. Receive the new state\n",
    "    - 새 상태를 받기\n",
    "\n",
    "\n",
    "Here's our random agent script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53a10df-f58b-44b9-b62d-8f4bea2782fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 out of 5\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "step: 1 out of 5\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "step: 2 out of 5\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "step: 3 out of 5\n",
      "+---------+\n",
      "|R: | :\u001b[43m \u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "step: 4 out of 5\n",
      "+---------+\n",
      "|R: | :\u001b[43m \u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "step: 5 out of 5\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5\n",
    "for s in range(num_steps+1):\n",
    "    print(f\"step: {s} out of {num_steps}\")\n",
    "\n",
    "    # sample a random action from the list of available actions\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # perform this action on the environment\n",
    "    env.step(action)\n",
    "\n",
    "    # print the new state\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c7d0a-9724-4443-b6b4-4eac1c184693",
   "metadata": {},
   "source": [
    "end this instance of the taxi environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f5e17-1d21-4526-adb3-4eacca691fe6",
   "metadata": {},
   "source": [
    "A Q-table is simply a look-up table storing values representing the maximum expected future rewards our agent can expect for a certain action in a certain state (known as Q-values).\n",
    "- Q table이란 에이전트가 특정 상태에서 특정 행동에 기대할 수 있는 최대 기대 보상을 나타내는 값을 wjwkdgkms 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d90382-cada-488c-b14b-e554e5350301",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.n  # total number of states (S)\n",
    "action_size = env.action_space.n      # total number of actions (A)\n",
    "\n",
    "# initialize a qtable with 0's for all Q-values\n",
    "qtable = np.zeros((state_size, action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc3167-3e7a-4d5b-b05d-873cd7655931",
   "metadata": {},
   "source": [
    "`-` Q-learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14918b93-b106-4d8f-a5bf-7f0971cbad4f",
   "metadata": {},
   "source": [
    "The Q-learning algorithm will help our agent update the current Q-value $(Q(S_t,A_t))$ with its observations after taking an action. I.e. increase Q if it encountered a positive reward, or decrease Q if it encountered a negative one.\n",
    "- Q학습 알고리즙은 에이전트가 행동을 한 후에 관찰한 것으로 현재 Q값$(Q(S_t,A_t))$을 업데이트하는데 도움이 된다.\n",
    "- 긍정적인 보상이 있으면 Q가 증가하고, 부정적인 보상이 있으면 Q가 감소한다.\n",
    "\n",
    "Note that in Taxi, our agent doesn't receive a positive reward until it successfully drops off a passenger (+20 points). Hence even if our agent is heading in the correct direction, there will be a delay in the positive reward it should receive.\n",
    "- 택시에서 에이전트는 승객이 성공적으로 내릴때까지 긍정적인 보상을 받지 않는다.\n",
    "- 그래서 옳은 방향으로 가고 있다고 해도 보상을 받지 못하고 지연됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc93f7-2269-4f9d-80ae-642b3df40f0f",
   "metadata": {},
   "source": [
    "$$\\gamma max_a Q(S_{t+1},a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c5f1c-86fd-4280-a6da-fe3e3f5f0604",
   "metadata": {},
   "source": [
    "This term adjusts our current Q-value to include a portion of the rewards it may receive sometime in the future (St+1). The 'a' term refers to all the possible actions available for that state. The equation also contains two hyperparameters which we can specify:\n",
    "- 미래에 받을 수 있는 보상의 일부를 포함하도록 현재 Q값을 조정한다.\n",
    "\n",
    "Learning rate (α): how easily the agent should accept new information over previously learnt information\n",
    "- 얼마나 쉽게 에이전트가 이전에 학습된 정보보다 새로운 정보를 받아야 하는지\n",
    "\n",
    "Discount factor (γ): how much the agent should take into consideration the rewards it could receive in the future versus its immediate reward\n",
    "- 에이전트가 즉각적인 보상 대비 미래에 받을 수 있는 보상을 얼마나 고려해야 하는지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7884b55-8cbb-4d57-a1e4-8ca8f099d531",
   "metadata": {},
   "source": [
    "Q algorithm 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa7764-b9f6-4f46-8940-dfdee3f23c62",
   "metadata": {},
   "source": [
    "```python\n",
    "learning_rate = 0.9\n",
    "discount_rate = 0.8\n",
    "\n",
    "# Qlearning algorithm: Q(s,a) := Q(s,a) + learning_rate * (reward + discount_rate * max Q(s',a') - Q(s,a))\n",
    "qtable[state, action] += learning_rate * (reward + discount_rate * np.max(qtable[new_state,:]) - qtable[state,action])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee78a0b-dbb4-416a-9de8-500cca078e0a",
   "metadata": {},
   "source": [
    "We can let our agent explore to update our Q-table using the Q-learning algorithm. As our agent learns more about the environment, we can let it use this knowledge to take more optimal actions and converge faster - known as exploitation.\n",
    "- 익스플로잇exploitation: 에이전트가 환경에 대해 더 많이 알게 되면 이 지식을 사용하여 더 최적의 조치를 취하고 더 빠르게 수렴할 수 있도록 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efd725-d1b8-4139-a31d-e37ba3c3ddc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "```python\n",
    "# exploration-exploitation tradeoff\n",
    "epsilon = 1.0     # probability that our agent will explore\n",
    "\n",
    "decay_rate = 0.01 # of epsilon\n",
    "\n",
    "if random.uniform(0,1) < epsilon:\n",
    "    # explore\n",
    "    action = env.action_space.sample()\n",
    "else:\n",
    "    # exploit\n",
    "    action = np.argmax(qtable[state,:])\n",
    "\n",
    "# epsilon decreases exponentially --> our agent will explore less and less\n",
    "epsilon = np.exp(-decay_rate*episode)\n",
    "# 각 단계마다 기하급수적으로 감소하므로 에이전트가 시간이 지남에 따라 탐색하는 횟수가 줄어듭니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca51dec-9001-403c-a1f4-f1add2daed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed over 1000 episodes\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to watch trained agent... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINED AGENT\n",
      "Step 1\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "score: -1\n",
      "TRAINED AGENT\n",
      "Step 2\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "score: -2\n",
      "TRAINED AGENT\n",
      "Step 3\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "score: -3\n",
      "TRAINED AGENT\n",
      "Step 4\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "score: -4\n",
      "TRAINED AGENT\n",
      "Step 5\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "score: -5\n",
      "TRAINED AGENT\n",
      "Step 6\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "score: -6\n",
      "TRAINED AGENT\n",
      "Step 7\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "score: -7\n",
      "TRAINED AGENT\n",
      "Step 8\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "score: -8\n",
      "TRAINED AGENT\n",
      "Step 9\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "score: -9\n",
      "TRAINED AGENT\n",
      "Step 10\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "score: -10\n",
      "TRAINED AGENT\n",
      "Step 11\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "score: -11\n",
      "TRAINED AGENT\n",
      "Step 12\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "score: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "def main():\n",
    "\n",
    "    # create Taxi environment\n",
    "    env = gym.make('Taxi-v3')\n",
    "\n",
    "    # initialize q-table\n",
    "    state_size = env.observation_space.n\n",
    "    action_size = env.action_space.n   \n",
    "    qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "    # hyperparameters\n",
    "    learning_rate = 0.9\n",
    "    discount_rate = 0.8\n",
    "    epsilon = 1.0\n",
    "    decay_rate= 0.005\n",
    "\n",
    "    # training variables\n",
    "    num_episodes = 1000\n",
    "    max_steps = 99 # per episode\n",
    "\n",
    "    # training\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        for s in range(max_steps):\n",
    "\n",
    "            # exploration-exploitation tradeoff\n",
    "            if random.uniform(0,1) < epsilon:\n",
    "                # explore\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # exploit\n",
    "                action = np.argmax(qtable[state,:])\n",
    "\n",
    "            # take action and observe reward\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Q-learning algorithm\n",
    "            qtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
    "\n",
    "            # Update to our new state\n",
    "            state = new_state\n",
    "\n",
    "            # if done, finish episode\n",
    "            if done == True:\n",
    "                break\n",
    "\n",
    "        # Decrease epsilon\n",
    "        epsilon = np.exp(-decay_rate*episode)\n",
    "\n",
    "    print(f\"Training completed over {num_episodes} episodes\")\n",
    "    input(\"Press Enter to watch trained agent...\")\n",
    "\n",
    "    # watch trained agent\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = 0\n",
    "\n",
    "    for s in range(max_steps):\n",
    "\n",
    "        print(f\"TRAINED AGENT\")\n",
    "        print(\"Step {}\".format(s+1))\n",
    "\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        rewards += reward\n",
    "        env.render()\n",
    "        print(f\"score: {rewards}\")\n",
    "        state = new_state\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9ad0c-e23d-47ca-9f2b-a1c964c09362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
